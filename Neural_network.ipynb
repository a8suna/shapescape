{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26400dad",
   "metadata": {},
   "source": [
    "# This code is to make a Pytorch model of a basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5b09e5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Version: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch version 2.9.0+cpu\n",
      "Torchvision version 0.24.0+cpu\n",
      "Numpy version 2.3.4\n",
      "Pandas version 2.2.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt # For data viz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print('System Version:', sys.version)\n",
    "print('PyTorch version', torch.__version__)\n",
    "print('Torchvision version', torchvision.__version__)\n",
    "print('Numpy version', np.__version__)\n",
    "print('Pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "12267a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the class for the data set, setting up the initation, \n",
    "# Length of the dataset\n",
    "# And the get item\n",
    "\n",
    "class ShapeDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None): # This initates the data\n",
    "        self.data = ImageFolder(data_dir, transform=transform)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) #checks the length of the data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx] # This is for the labelling the idx is the tag.\n",
    "    \n",
    "    @property\n",
    "    def classes(self): # returns all the data of the class\n",
    "        return self.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2206051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load in the data set with different shapes\n",
    "\n",
    "Dataset = ShapeDataset(\n",
    "    data_dir='Shapes'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "98821bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dataset) # This checks how many images there are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "d7a1a8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x200>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label = Dataset[6]\n",
    "print(label)\n",
    "image # To check the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2809137",
   "metadata": {},
   "source": [
    "# We are then going to resize the images to a smaller size and start with a smaller neural network maybe of 6 shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9cfb51",
   "metadata": {},
   "source": [
    "    So we can use a dataloader and format the files to work for our specific needs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57e202",
   "metadata": {},
   "source": [
    "So the first part of the code we have to practice trying to input the data, so i might just do one with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "85f8910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the data from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6b00548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#uses the transform function to be able to \n",
    "train_folder = 'Shapes\\Train'\n",
    "valid_folder = 'Shapes\\Valid'\n",
    "test_folder = 'Shapes\\Test'\n",
    "\n",
    "train_dataset = ShapeDataset(train_folder, transform=transform)\n",
    "val_dataset = ShapeDataset(valid_folder, transform=transform)\n",
    "test_dataset = ShapeDataset(test_folder, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3, shuffle=False)\n",
    "test_loader = DataLoader(val_dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d01ee1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=200x200 at 0x2571B95FA70>\n"
     ]
    }
   ],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "78e77ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=2352, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9e65d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) # This goes and adjusts the model using loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "755a904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        print(X.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "99e50da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "7185b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 2.256374  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 2.122061 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 2.152141  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.995548 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 2.093723  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.859799 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.848426  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.706653 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.691910  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 1.552476 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.596900  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 1.417256 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.463341  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 1.308619 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.287678  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 1.229356 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.315580  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.163621 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.187777  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 1.110445 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.112082  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 1.068210 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 0.989346  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 1.042138 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.076845  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 1.018431 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.045511  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.974941 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 0.824253  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.967147 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 0.858223  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.936762 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.128505  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.939764 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 0.979808  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.887728 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 1.092545  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.894150 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "torch.Size([3, 3, 28, 28])\n",
      "loss: 0.799929  [    3/    9]\n",
      "torch.Size([3, 3, 28, 28])\n",
      "torch.Size([3, 3, 28, 28])\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.913098 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a29e16a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\") # So you export the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "6f59f61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True)) # Load the Model in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "379aa54d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x784 and 2352x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[287], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     12\u001b[0m     predicted, actual \u001b[38;5;241m=\u001b[39m classes[pred[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m0\u001b[39m)], classes[y]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[280], line 19\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m---> 19\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_relu_stack(x)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\carlo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x784 and 2352x512)"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"Circle\",\n",
    "     \"Square\",\n",
    "     \"Triangle\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_dataset[0][0], test_dataset[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
